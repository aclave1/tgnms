#!/usr/bin/env python3

""" LinkInsight class provides methods to query stats, compute link
    mean/variance, and write back the computed stats back to Beringei DB.
"""

import numpy as np
import sys
import os
import json
import time

# Include the data structures generated by thrift
sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname(__file__), "..") + "/interface/gen-py")
)
from facebook.gorilla.beringei_query import ttypes as bq
from facebook.gorilla.Topology.ttypes import Topology

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from module.topology_handler import TopologyHelper
from module.beringei_db_access import BeringeiDbAccess


class LinkInsight(object):
    """Include functions for link-level insight calculation. Data is obtained
       from Beringei database and the computed stats is write back to Beringei
       database.
    """

    def get_network_wide_link_key_id_by_metric(
        self, metric, network_config, topology_name="tower G"
    ):
        """Send query to Beringei Query Server for metrics Beringei key_id.
           This function is supposed for debugging. For formal link query, use
           ConstructQueryRequest and provide metric information
           (source_mac, peer_mac, topology_name, metric_name)
           to Beringei Query Server, which is also much faster.

        Args:
        metric: metric to query, like 'phystatus.snrest'.
        network_config: network config dictionary, have keys of
        "link_macs_to_name", "node_mac_to_name", and "node_mac_to_site".
        topology_name: name of the setup, like "tower G".

        Return:
        key_id_to_macs: dict from link metric key_id to link_macs, which is the
                        tuple of (source_mac, peer_mac), on success.
                        On error, return empty dict.
        """

        # Construct the Beringei database key_id to link macs based on MySQL table,
        # the link macs is the tuple of (source_mac, peer_mac)
        beringei_db_access = BeringeiDbAccess()
        if beringei_db_access is None:
            print("Fail to create BeringeiDbAccess object")
            return {}

        key_id_to_macs = {}
        num_links = len(network_config["link_macs_to_name"].keys())
        print("In total, {} links to find key_ids".format(num_links))
        for source_mac, peer_mac in network_config["link_macs_to_name"].keys():

            full_key = "tgf.{}".format(peer_mac) + "." + metric

            # Construct request to send to obtain Beringei key_id
            type_ahead_request = bq.TypeAheadRequest(
                topologyName=topology_name, input=full_key
            )

            # Read key_id via Beringei Query Server
            try:
                return_key_id = beringei_db_access.get_beringei_key_id(
                    source_mac, type_ahead_request
                )
            except ValueError as err:
                print("Get Beringei key_id error:", err.args)
                return {}

            key_id_to_macs[return_key_id] = (source_mac, peer_mac)
        return key_id_to_macs

    def construct_write_request(
        self,
        link_stats,
        metric_name,
        sample_duration_in_s,
        source_db_interval,
        stats_query_timestamp,
        topology_name="tower G",
        dest_db_interval=30,
    ):
        """Prepare computed link stats request to be send to Beringei Query Server
           for Beringei database writing.

        Args:
        link_stats: computed link_stats dict from compute_link_avg_and_var().
        metric_name: name of the metric to query, like "phystatus.ssnrest".
        sample_duration_in_s: the duration of the stats read, for example 3600
            means link data of past 1 hour are used to computed the link stats.
        source_db_interval: interval of the Beringei database reading from.
        stats_query_timestamp: the time on which the link stats is computed.
        topology_name: name of the setup, like "tower G".
        dest_db_interval: indicates which Beringei DataBase to write to.

        Return:
        stats_to_write: On success, stats to write to the Beringei database,
                        type StatsWriteRequest. Raise exception on error.
        """
        topology = Topology(name=topology_name)
        if topology is None:
            raise ValueError("Cannot create topology object")

        query_agents = []
        for link_idx in link_stats:
            per_link_stats = link_stats[link_idx]
            if per_link_stats["mean"] is None:
                print(
                    "Skipping link between {} and {} due to no data".format(
                        per_link_stats["source_mac"], per_link_stats["peer_mac"]
                    )
                )
                continue
            var_stats = bq.Stat(
                key=(
                    "{}.{}.{}.{}.{}".format(
                        per_link_stats["peer_mac"],
                        metric_name,
                        "variance",
                        sample_duration_in_s,
                        source_db_interval,
                    )
                ),
                ts=stats_query_timestamp,
                value=per_link_stats["variance"],
            )
            var_node_state = bq.NodeStates(
                mac=per_link_stats["source_mac"],
                site=per_link_stats["source_site"],
                name=per_link_stats["source_name"],
                stats=[var_stats],
            )
            query_agents.append(var_node_state)
            mean_stats = bq.Stat(
                key=(
                    "{}.{}.{}.{}.{}".format(
                        per_link_stats["peer_mac"],
                        metric_name,
                        "mean",
                        sample_duration_in_s,
                        source_db_interval,
                    )
                ),
                ts=stats_query_timestamp,
                value=per_link_stats["mean"],
            )
            mean_node_state = bq.NodeStates(
                mac=per_link_stats["source_mac"],
                site=per_link_stats["source_site"],
                name=per_link_stats["source_name"],
                stats=[mean_stats],
            )
            query_agents.append(mean_node_state)

        stats_to_write = bq.StatsWriteRequest(
            topology=topology, agents=query_agents, interval=dest_db_interval
        )
        return stats_to_write

    def construct_query_request(
        self,
        link_macs_list=None,
        topology_name="tower G",
        metric_name="SNR",
        key_option="link_metric",
        key_ids=None,
        start_ts=None,
        end_ts=None,
        source_db_interval=30,
    ):
        """Prepare query request by a). key_id or b). link_macs and metric_name. The
           constructed queries will be sent to Beringei Query Server for raw stats
           reading by BeringeiDbAccess.read_beringei_db.

        Args:
        query_keys: list of Beringei metrics list,
                    each element if of (source_mac, peer_mac).
        topology_name: name of the topology.
        metric_string: field of interest, like "phystatus.ssnrest".
        key_option: Can be either "link_metric" or "key_id".
                    On "link_metric", use the key metric, link_macs, and
                    topology_name to identify the right time series.
                    On "key_id", use key_ids.
        key_ids: list of Beringei metrics key_id or dict with key being key_id,
                 only used once key_option is "key_id"
        start_ts: query window left range.
        end_ts: query window right range.
        source_db_interval: the interval of the Beringei database that holds the
                            raw data.

        Return:
        query_request_to_send: query request to send, of type RawReadQueryRequest.
        """

        if end_ts is None:
            current_time_in_s = int(time.time())
            end_ts = current_time_in_s
        if start_ts is None:
            # default window is 1 hour
            start_ts = end_ts - 60 * 60

        # Construct the queries to send
        query_requests_to_send = []
        if key_option == "link_metric" and link_macs_list:
            for source_mac, peer_mac in link_macs_list:
                raw_query_key = bq.RawQueryKey(
                    sourceMac=source_mac,
                    peerMac=peer_mac,
                    metricName=metric_name,
                    topologyName=topology_name,
                )
                query_to_send = bq.RawReadQuery(
                    queryKeyList=[raw_query_key],
                    startTimestamp=start_ts,
                    endTimestamp=end_ts,
                    interval=source_db_interval,
                )
                query_requests_to_send.append(query_to_send)
        elif key_option == "key_id" and key_ids:
            for key_id in key_ids:
                raw_query_key = bq.RawQueryKey(key_id=key_id)
                query_to_send = bq.RawReadQuery(
                    queryKeyList=[raw_query_key],
                    startTimestamp=start_ts,
                    endTimestamp=end_ts,
                    interval=source_db_interval,
                )
                query_requests_to_send.append(query_to_send)
        else:
            print("Nothing to query")

        # Generate the query list to send
        query_request_to_send = bq.RawReadQueryRequest(query_requests_to_send)

        return query_request_to_send

    def compute_link_avg_and_var(
        self,
        query_returns,
        query_request_to_send,
        network_config,
        dump_to_json=False,
        json_log_name="sample_log.json",
    ):
        """Compute the link metric average and variance, optionally log to JSON.

        Args:
        query_returns: query returned results, type RawQueryReturn.
        query_request_to_send: the query sent to the BQS, used for the macs
                               and metric name creation for link_stats.
        key_id_to_link_macs: dict, Beringei key_id to link_macs,
                             which is the tuple of (source_mac, peer_mac).
        network_config: network config obtained from the api service.
        dump_to_json: If True, save a copy of the link stats to JSON;
                      If False, don't save to JSON.
        json_log_name: the name of the output JSON file.

        Return:
        link_stats: a dict, link idx as key, each key maps to a sub-dict with
                    keys of "linkname", "beringei_key_id", "shard_id",
                    "source_mac", "peer_mac", "num_data_points",
                    "variance", "mean".
        """

        # Compute the stats and LOG to JSON
        link_stats = {}
        query_return_list = query_returns.queryReturnList
        num_no_report_links = 0
        for return_idx, query_return in enumerate(query_return_list):
            # By construct, each link query request will have only 1 query_key
            query_return = query_return.timeSeriesAndKeyList[0]
            reported_values = [
                time_value_pair.value for time_value_pair in query_return.timeSeries
            ]
            key_id = query_return.beringeiDBKey.key
            shard_id = query_return.beringeiDBKey.shardId
            query_key = query_request_to_send.queries[return_idx].queryKeyList[0]
            source_mac = query_key.sourceMac
            peer_mac = query_key.peerMac

            # generate the link status of each link
            per_link_stats = {}
            per_link_stats["linkname"] = network_config["link_macs_to_name"][
                source_mac, peer_mac
            ]
            per_link_stats["beringei_key_id"] = int(key_id)
            per_link_stats["shard_id"] = int(shard_id)
            per_link_stats["source_mac"] = source_mac
            per_link_stats["peer_mac"] = peer_mac
            per_link_stats["source_site"] = network_config["node_mac_to_site"][
                source_mac
            ]
            per_link_stats["source_name"] = network_config["node_mac_to_name"][
                source_mac
            ]
            if not reported_values:
                # Count the number of links without any report.
                # Put None for all stats
                num_no_report_links += 1
                per_link_stats["num_data_points"] = 0
                per_link_stats["variance"] = None
                per_link_stats["mean"] = None
            else:
                # If there is data report for the link,
                # log the mean and variance
                per_link_stats["num_data_points"] = len(reported_values)
                per_link_stats["variance"] = np.var(reported_values)
                per_link_stats["mean"] = np.mean(reported_values)

            link_stats[return_idx] = per_link_stats

        print(
            "{} out of {} links do not have any report".format(
                num_no_report_links, len(query_return_list)
            )
        )

        if dump_to_json:
            # Log to JSON file only if dump_to_json is True
            print("Logging to " + json_log_name)

            try:
                with open(json_log_name, "w", encoding="utf-8") as jsonfile:
                    json.dump(
                        link_stats,
                        jsonfile,
                        sort_keys=True,
                        indent=4,
                        separators=(",", ": "),
                    )
            except Exception:
                print("Cannot open JSON file to write")

        return link_stats
